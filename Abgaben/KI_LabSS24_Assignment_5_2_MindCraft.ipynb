{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bde72f-afa0-4c5a-9e0e-05a4397575c7",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6176cb0-8914-47e0-9361-273b3d3a0b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efecc181e10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "from heapq import nlargest\n",
    "from operator import itemgetter\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d372d0b-6605-4690-8f23-23b8aeccd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "test_data = []\n",
    "word_to_ix = {}\n",
    "word_num = {}\n",
    "#pos_to_ix = {}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "VOCAB_LIMIT = 5000\n",
    "\n",
    "labels = {\"anger\": 0, \"joy\": 1, \"optimism\": 2, \"sadness\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5cd639-17f8-463a-a8f1-1fc272416774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lines: 3257\n",
      "Training labels: 3257\n",
      "Test lines: 1421\n",
      "Test labels: 1421\n"
     ]
    }
   ],
   "source": [
    "with open('/home/ruwen/tweeteval/datasets/emotion/train_text.txt') as f:\n",
    "    t_lines = f.read().splitlines()\n",
    "\n",
    "with open('/home/ruwen/tweeteval/datasets/emotion/test_text.txt') as f:\n",
    "    test_lines = f.read().splitlines()\n",
    "\n",
    "with open('/home/ruwen/tweeteval/datasets/emotion/train_labels.txt') as f:\n",
    "    train_labels = f.read().splitlines()\n",
    "\n",
    "train_labels = [int(numeric_string) for numeric_string in train_labels]\n",
    "\n",
    "with open('/home/ruwen/tweeteval/datasets/emotion/test_labels.txt') as f:\n",
    "    test_labels = f.read().splitlines()\n",
    "\n",
    "test_labels = [int(numeric_string) for numeric_string in test_labels]\n",
    "\n",
    "print(\"Training lines: \" + str(len(t_lines)))\n",
    "print(\"Training labels: \" + str(len(train_labels)))\n",
    "print(\"Test lines: \" + str(len(test_lines)))\n",
    "print(\"Test labels: \" + str(len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e3c273-fe81-478b-ae21-d7774d07f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(t_list, lines, labels):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        doc = nlp(line)\n",
    "    \n",
    "        t_list.append(([], []))\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.text not in word_to_ix:  # word has not been assigned an index yet\n",
    "                word_to_ix[token.text] = len(word_to_ix)  # Assign each word with a unique index\n",
    "\n",
    "            if token.text not in word_num:\n",
    "                word_num[token.text] = 1\n",
    "            else:\n",
    "                word_num[token.text] += 1\n",
    "            \n",
    "            t_list[-1][0].append(token.text)\n",
    "            t_list[-1][1].append(labels[i])\n",
    "    \n",
    "        #print(\"line finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bd4e60-8583-4fba-b0a1-9ffb4808c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(training_data, t_lines, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb5deed-f397-4c3c-ac6e-fec34c388187",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(test_data, test_lines, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ee64e1-5311-4b99-b91a-103a455ab889",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = nlargest(VOCAB_LIMIT, word_num, key=word_num.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0af84c8-a7e3-42b9-8e2f-5a7050388fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict(sorted(word_num.items(), key=itemgetter(1), reverse=True)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658df6e-37e8-4e30-9135-acfd940a1e42",
   "metadata": {},
   "source": [
    "### LSTM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d25f969-db17-44a4-9086-de44f17ca209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94c68d7-5f5e-437d-9217-8fea03e4ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_top(seq):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if w in top_words:\n",
    "            idxs.append(top_words.index(w))\n",
    "        else:\n",
    "            idxs.append(5000)\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f9b74d-147d-4dc6-9605-788d91f026c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8157f49-ffc0-4972-a113-17fbeef6597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, 5001, 4)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b4e2aa8-3c29-47ca-a1e0-1cf91fe04a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    for epoch in range(epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        for sentence, tags in training_data:\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "    \n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence_top(sentence)\n",
    "            targets = torch.tensor(tags, dtype=torch.long)\n",
    "            #targets = prepare_sequence(tags, pos_to_ix)\n",
    "    \n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "    \n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores[-1], targets[-1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Epoch \" +  str(epoch + 1) + \" loss: \" + str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3107225f-6e02-4ca4-ac90-4572e8a6d0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.458427757024765\n",
      "Epoch 2 loss: 0.16824369132518768\n",
      "Epoch 3 loss: 0.01595539227128029\n",
      "Epoch 4 loss: 0.9161590933799744\n",
      "Epoch 5 loss: 0.00021789084712509066\n"
     ]
    }
   ],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cb480d8-e6a0-4daf-84c2-308187471f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in test_data:\n",
    "\n",
    "            sentence_in = prepare_sequence_top(sentence)\n",
    "            targets = torch.tensor(tags, dtype=torch.long)\n",
    "        \n",
    "            tag_scores = model(sentence_in)\n",
    "            #print(tag_scores.data)\n",
    "            #print(tag_scores.data[-1])\n",
    "            #print(torch.max(tag_scores.data, 1)[1][-1])\n",
    "            #print(tags)\n",
    "                \n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(tag_scores.data, 1)\n",
    "            #print(predicted[-1])\n",
    "            #print(tags[-1])\n",
    "            correct += (predicted[-1] == tags[-1]).sum().item()\n",
    "            #print(correct)\n",
    "            \n",
    "            \n",
    "            total += 1\n",
    "    \n",
    "    print(f'Accuracy of the network on the test data: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0214d53-99a9-4e78-aa81-0df49b7478fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data: 50 %\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5519d-1f75-489b-a2eb-67d8c9c3ef91",
   "metadata": {},
   "source": [
    "### GRU Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bf20762-df87-4809-8d2b-0f12d22323c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(GRUTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dec9ef26-9923-4c5b-907d-7f17c95efaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUTagger(EMBEDDING_DIM, HIDDEN_DIM, 5001, 4)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2099cad-ce19-4d2a-ad9f-3bf97eb06fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.066338062286377\n",
      "Epoch 2 loss: 1.042771816253662\n",
      "Epoch 3 loss: 1.0505871772766113\n",
      "Epoch 4 loss: 1.072179913520813\n",
      "Epoch 5 loss: 1.1017217636108398\n",
      "Epoch 6 loss: 1.1343740224838257\n",
      "Epoch 7 loss: 1.1660730838775635\n",
      "Epoch 8 loss: 1.1940438747406006\n",
      "Epoch 9 loss: 1.216921091079712\n",
      "Epoch 10 loss: 1.2344927787780762\n",
      "Epoch 11 loss: 1.247281551361084\n",
      "Epoch 12 loss: 1.256147861480713\n",
      "Epoch 13 loss: 1.262015461921692\n",
      "Epoch 14 loss: 1.265702724456787\n",
      "Epoch 15 loss: 1.2678502798080444\n",
      "Epoch 16 loss: 1.2689030170440674\n"
     ]
    }
   ],
   "source": [
    "train(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "351574f3-0478-4636-9e96-0fe68ba7404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data: 49 %\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6fb2f-be57-4f1b-b082-fc5dbfa55d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
